{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上分思路\n",
    "- 数据\n",
    "- 模型\n",
    "- 损失函数\n",
    "- 训练方式\n",
    "- 超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第一步**\n",
    "在运行环境中安装对应的库 执行该命令即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第二步**\n",
    "导入运行所需要的库函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第三步**\n",
    "数据集路径配置设置\n",
    "- 比赛的数据部分分为**数据特征**和**数据真值**两部分，数据特征是模型训练的**输入**，数据真值是模型训练的**标签**\n",
    "- 其中数据特征部分 输入的路径目录下包含年份文件夹 \n",
    " - 例如示例给出的 \"输入路径/2021/...\" 各年份文件夹下包含从官网下载的压缩包(e.g. weather.round1.train.ft.2021.1.zip) 解压后文件夹下有不同时段的数据文件夹(e.g. 20210101-00), 内部包含6个nc文件, 是从伏羲大模型中获取的从第6小时到第72小时的数据\n",
    "\n",
    "- 数据真值部分 输入的路径目录下包含3个年份的.nc数据, 其中选择哪些年份的特征数据作为输入, 就在years中添加哪些年份\n",
    "- fcst_steps指预测的时间步长, 从第1小时到第72小时, 间隔为1小时\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path config\n",
    "feature_path = '/ai_share/caozhibin/tianchi_precipatition_prediction/train' #自定义路径并修改为自己的路径\n",
    "gt_path = '/ai_share/caozhibin/tianchi_precipatition_prediction/groundtruth' #自定义路径并修改为自己的路径\n",
    "years = ['2021']\n",
    "fcst_steps = list(range(1, 73, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第四步**\n",
    "Feature类和GroundTruth类是数据集的定义\n",
    "方便后续自定义数据集和数据加载类, 方便我们训练时取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature部分\n",
    "class Feature:\n",
    "    def __init__(self):\n",
    "        self.path = feature_path\n",
    "        self.years = years\n",
    "        self.fcst_steps = fcst_steps\n",
    "        self.features_paths_dict = self.get_features_paths()\n",
    "\n",
    "    def get_features_paths(self):\n",
    "        init_time_path_dict = {}\n",
    "        for year in self.years:\n",
    "            init_time_dir_year = os.listdir(os.path.join(self.path, year))\n",
    "            for init_time in sorted(init_time_dir_year):\n",
    "                init_time_path_dict[pd.to_datetime(init_time)] = os.path.join(self.path, year, init_time)\n",
    "        return init_time_path_dict\n",
    "\n",
    "    def get_fts(self, init_time):\n",
    "        return xr.open_mfdataset(self.features_paths_dict.get(init_time) + '/*').sel(lead_time=self.fcst_steps).isel(\n",
    "            time=0)\n",
    "    \n",
    "# GroundTruth部分\n",
    "class GT:\n",
    "    def __init__(self):\n",
    "        self.path = gt_path\n",
    "        self.years = years\n",
    "        self.fcst_steps = fcst_steps\n",
    "        self.gt_paths = [os.path.join(self.path, f'{year}.nc') for year in self.years]\n",
    "        self.gts = xr.open_mfdataset(self.gt_paths)\n",
    "\n",
    "    def parser_gt_timestamps(self, init_time):\n",
    "        return [init_time + pd.Timedelta(f'{fcst_step}h') for fcst_step in self.fcst_steps]\n",
    "\n",
    "    def get_gts(self, init_time):\n",
    "\n",
    "        return self.gts.sel(time=self.parser_gt_timestamps(init_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第五步**\n",
    "mydataset类的定义, 整合了加载特征和特征对应真值的功能, 方便后续训练时取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Dataset部分\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ft = Feature()\n",
    "        self.gt = GT()\n",
    "        self.features_paths_dict = self.ft.features_paths_dict\n",
    "        self.init_times = list(self.features_paths_dict.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        init_time = self.init_times[index]\n",
    "        try:\n",
    "            ft_item = self.ft.get_fts(init_time).to_array().isel(variable=0).values\n",
    "            gt_item = self.gt.get_gts(init_time).to_array().isel(variable=0).values\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            print(f'init_time: {init_time} not found')\n",
    "            # return None, None\n",
    "            return self.__getitem__(index - 1)\n",
    "        \n",
    "        return ft_item, gt_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.init_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第六步**\n",
    "前五步已经完成了数据预处理加载的相关类和函数的准备, 这里我们可以通过实例化mydataset类来查看数据数量\n",
    "同时完成数据集的构建后, 我们可以通过DataLoader来查看数据集的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample num: 724\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# define dataset\n",
    "my_data = mydataset()\n",
    "print('sample num:', mydataset().__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(my_data))\n",
    "val_size = len(my_data) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(my_data, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第七步**\n",
    "- 完成了数据的准备工作, 接下来就是构建模型的部分\n",
    "- Model这个类, 对我们的模型进行定义, 方便后续训练时调用\n",
    "- 这里我们以一个简单的只有一个卷积层的网络为例\n",
    "- 在本次比赛中, 我们的输入数据维度是(1, 24, 72, W, H), 输出数据维度是(1, 72, W, H) 可以在赛题中查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 实验1 加深模型\n",
    "class EnhancedModel(nn.Module):\n",
    "    def __init__(self, num_in_ch, num_out_ch):\n",
    "        super(EnhancedModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_in_ch, 64, kernel_size=3, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, num_out_ch, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, S, C, W, H = tuple(x.shape)\n",
    "        x = x.reshape(B, -1, W, H)\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchnorm(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.batchnorm(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv4(out)\n",
    "        out = out.reshape(B, S, W, H)\n",
    "        return out\n",
    "    \n",
    "# define model\n",
    "in_varibales = 24\n",
    "in_times = len(fcst_steps)\n",
    "out_varibales = 1\n",
    "out_times = len(fcst_steps)\n",
    "input_size = in_times * in_varibales\n",
    "output_size = out_times * out_varibales\n",
    "model = EnhancedModel(input_size, output_size).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第八步**\n",
    "定义模型的损失函数部分， 用于模型训练做反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第九步**\n",
    "模型训练部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/579], Loss: 0.714885\n",
      "Epoch [1/10], Step [20/579], Loss: 1.115040\n",
      "Epoch [1/10], Step [30/579], Loss: 0.331170\n",
      "Epoch [1/10], Step [40/579], Loss: 0.233220\n",
      "Epoch [1/10], Step [50/579], Loss: 0.959460\n",
      "Epoch [1/10], Step [60/579], Loss: 0.128695\n",
      "Epoch [1/10], Step [70/579], Loss: 0.141988\n",
      "Epoch [1/10], Step [80/579], Loss: 0.061162\n",
      "Epoch [1/10], Step [90/579], Loss: 1.044837\n",
      "Epoch [1/10], Step [100/579], Loss: 0.737786\n",
      "Epoch [1/10], Step [110/579], Loss: 0.421078\n",
      "Epoch [1/10], Step [120/579], Loss: 0.037219\n",
      "Epoch [1/10], Step [130/579], Loss: 0.290093\n",
      "Epoch [1/10], Step [140/579], Loss: 0.566893\n",
      "Epoch [1/10], Step [150/579], Loss: 0.104870\n",
      "Epoch [1/10], Step [160/579], Loss: 0.286806\n",
      "Epoch [1/10], Step [170/579], Loss: 0.358794\n",
      "Epoch [1/10], Step [180/579], Loss: 0.438482\n",
      "Epoch [1/10], Step [190/579], Loss: 0.520226\n",
      "Epoch [1/10], Step [200/579], Loss: 0.165980\n",
      "Epoch [1/10], Step [210/579], Loss: 0.310535\n",
      "Epoch [1/10], Step [220/579], Loss: 0.217066\n",
      "Epoch [1/10], Step [230/579], Loss: 0.208204\n",
      "Epoch [1/10], Step [240/579], Loss: 0.991095\n",
      "Epoch [1/10], Step [250/579], Loss: 0.133176\n",
      "Epoch [1/10], Step [260/579], Loss: 0.715028\n",
      "Epoch [1/10], Step [270/579], Loss: 0.145859\n",
      "Epoch [1/10], Step [280/579], Loss: 0.152401\n",
      "Epoch [1/10], Step [290/579], Loss: 0.038365\n",
      "Epoch [1/10], Step [300/579], Loss: 0.076728\n",
      "Epoch [1/10], Step [310/579], Loss: 0.085184\n",
      "Epoch [1/10], Step [320/579], Loss: 0.128865\n",
      "Epoch [1/10], Step [330/579], Loss: 0.155156\n",
      "Epoch [1/10], Step [340/579], Loss: 0.059766\n",
      "Epoch [1/10], Step [350/579], Loss: 0.142875\n",
      "Epoch [1/10], Step [360/579], Loss: 0.195163\n",
      "Epoch [1/10], Step [370/579], Loss: 0.272694\n",
      "Epoch [1/10], Step [380/579], Loss: 0.307330\n",
      "Epoch [1/10], Step [390/579], Loss: 0.716005\n",
      "Epoch [1/10], Step [400/579], Loss: 0.714610\n",
      "Epoch [1/10], Step [410/579], Loss: 0.060144\n",
      "Epoch [1/10], Step [420/579], Loss: 0.504175\n",
      "Epoch [1/10], Step [430/579], Loss: 1.046540\n",
      "Epoch [1/10], Step [440/579], Loss: 0.611814\n",
      "Epoch [1/10], Step [450/579], Loss: 0.116083\n",
      "Epoch [1/10], Step [460/579], Loss: 0.039743\n",
      "Epoch [1/10], Step [470/579], Loss: 0.404012\n",
      "Epoch [1/10], Step [480/579], Loss: 0.449282\n",
      "Epoch [1/10], Step [490/579], Loss: 0.480520\n",
      "Epoch [1/10], Step [500/579], Loss: 1.022136\n",
      "Epoch [1/10], Step [510/579], Loss: 0.662826\n",
      "Epoch [1/10], Step [520/579], Loss: 0.546786\n",
      "Epoch [1/10], Step [530/579], Loss: 0.021491\n",
      "Epoch [1/10], Step [540/579], Loss: 0.180979\n",
      "Epoch [1/10], Step [550/579], Loss: 0.089350\n",
      "Epoch [1/10], Step [560/579], Loss: 0.068239\n",
      "Epoch [1/10], Step [570/579], Loss: 0.083092\n",
      "[Epoch 1/10], Validation Loss: 0.003355\n",
      "Epoch [2/10], Step [10/579], Loss: 0.215246\n",
      "Epoch [2/10], Step [20/579], Loss: 0.500403\n",
      "Epoch [2/10], Step [30/579], Loss: 0.440403\n",
      "Epoch [2/10], Step [40/579], Loss: 0.656096\n",
      "Epoch [2/10], Step [50/579], Loss: 0.026281\n",
      "Epoch [2/10], Step [60/579], Loss: 0.245214\n",
      "Epoch [2/10], Step [70/579], Loss: 0.049283\n",
      "Epoch [2/10], Step [80/579], Loss: 1.052252\n",
      "Epoch [2/10], Step [90/579], Loss: 0.409364\n",
      "Epoch [2/10], Step [100/579], Loss: 0.518146\n",
      "Epoch [2/10], Step [110/579], Loss: 0.053671\n",
      "Epoch [2/10], Step [120/579], Loss: 0.092295\n",
      "Epoch [2/10], Step [130/579], Loss: 0.459371\n",
      "Epoch [2/10], Step [140/579], Loss: 0.170654\n",
      "Epoch [2/10], Step [150/579], Loss: 0.119346\n",
      "Epoch [2/10], Step [160/579], Loss: 0.787827\n",
      "Epoch [2/10], Step [170/579], Loss: 0.558466\n",
      "Epoch [2/10], Step [180/579], Loss: 0.059969\n",
      "Epoch [2/10], Step [190/579], Loss: 0.104345\n",
      "Epoch [2/10], Step [200/579], Loss: 0.114675\n",
      "Epoch [2/10], Step [210/579], Loss: 0.226428\n",
      "Epoch [2/10], Step [220/579], Loss: 0.559965\n",
      "Epoch [2/10], Step [230/579], Loss: 0.064338\n",
      "Epoch [2/10], Step [240/579], Loss: 0.088579\n",
      "Epoch [2/10], Step [250/579], Loss: 0.028769\n",
      "Epoch [2/10], Step [260/579], Loss: 0.082042\n",
      "Epoch [2/10], Step [270/579], Loss: 0.095356\n",
      "Epoch [2/10], Step [280/579], Loss: 0.079042\n",
      "Epoch [2/10], Step [290/579], Loss: 0.277359\n",
      "Epoch [2/10], Step [300/579], Loss: 0.414627\n",
      "Epoch [2/10], Step [310/579], Loss: 0.759270\n",
      "Epoch [2/10], Step [320/579], Loss: 0.024209\n",
      "Epoch [2/10], Step [330/579], Loss: 0.523742\n",
      "Epoch [2/10], Step [340/579], Loss: 0.595837\n",
      "Epoch [2/10], Step [350/579], Loss: 0.158026\n",
      "Epoch [2/10], Step [360/579], Loss: 0.072886\n",
      "Epoch [2/10], Step [370/579], Loss: 0.124468\n",
      "Epoch [2/10], Step [380/579], Loss: 0.365791\n",
      "Epoch [2/10], Step [390/579], Loss: 0.099856\n",
      "Epoch [2/10], Step [400/579], Loss: 0.212978\n",
      "Epoch [2/10], Step [410/579], Loss: 0.143640\n",
      "Epoch [2/10], Step [420/579], Loss: 0.789003\n",
      "Epoch [2/10], Step [430/579], Loss: 0.072441\n",
      "Epoch [2/10], Step [440/579], Loss: 0.058527\n",
      "Epoch [2/10], Step [450/579], Loss: 0.634218\n",
      "Epoch [2/10], Step [460/579], Loss: 0.340659\n",
      "Epoch [2/10], Step [470/579], Loss: 0.668290\n",
      "Epoch [2/10], Step [480/579], Loss: 0.489383\n",
      "Epoch [2/10], Step [490/579], Loss: 0.233562\n",
      "Epoch [2/10], Step [500/579], Loss: 0.044309\n",
      "Epoch [2/10], Step [510/579], Loss: 0.376875\n",
      "Epoch [2/10], Step [520/579], Loss: 0.023896\n",
      "Epoch [2/10], Step [530/579], Loss: 0.313838\n",
      "Epoch [2/10], Step [540/579], Loss: 0.545329\n",
      "Epoch [2/10], Step [550/579], Loss: 0.317371\n",
      "Epoch [2/10], Step [560/579], Loss: 0.633325\n",
      "Epoch [2/10], Step [570/579], Loss: 0.568022\n",
      "[Epoch 2/10], Validation Loss: 0.003329\n",
      "Epoch [3/10], Step [10/579], Loss: 0.467944\n",
      "Epoch [3/10], Step [20/579], Loss: 0.869704\n",
      "Epoch [3/10], Step [30/579], Loss: 0.031586\n",
      "Epoch [3/10], Step [40/579], Loss: 0.908473\n",
      "Epoch [3/10], Step [50/579], Loss: 0.313440\n",
      "Epoch [3/10], Step [60/579], Loss: 0.604147\n",
      "Epoch [3/10], Step [70/579], Loss: 0.818135\n",
      "Epoch [3/10], Step [80/579], Loss: 0.278224\n",
      "Epoch [3/10], Step [90/579], Loss: 0.075845\n",
      "Epoch [3/10], Step [100/579], Loss: 0.016123\n",
      "Epoch [3/10], Step [110/579], Loss: 0.485332\n",
      "Epoch [3/10], Step [120/579], Loss: 0.544201\n",
      "Epoch [3/10], Step [130/579], Loss: 0.060117\n",
      "Epoch [3/10], Step [140/579], Loss: 0.071795\n",
      "Epoch [3/10], Step [150/579], Loss: 0.067424\n",
      "Epoch [3/10], Step [160/579], Loss: 0.450895\n",
      "Epoch [3/10], Step [170/579], Loss: 0.167898\n",
      "Epoch [3/10], Step [180/579], Loss: 0.203710\n",
      "Epoch [3/10], Step [190/579], Loss: 0.114769\n",
      "Epoch [3/10], Step [200/579], Loss: 0.027904\n",
      "Epoch [3/10], Step [210/579], Loss: 0.183453\n",
      "Epoch [3/10], Step [220/579], Loss: 0.591825\n",
      "Epoch [3/10], Step [230/579], Loss: 0.035867\n",
      "Epoch [3/10], Step [240/579], Loss: 0.782097\n",
      "Epoch [3/10], Step [250/579], Loss: 0.607294\n",
      "Epoch [3/10], Step [260/579], Loss: 0.035982\n",
      "Epoch [3/10], Step [270/579], Loss: 0.424290\n",
      "Epoch [3/10], Step [280/579], Loss: 0.676620\n",
      "Epoch [3/10], Step [290/579], Loss: 0.604450\n",
      "Epoch [3/10], Step [300/579], Loss: 0.586436\n",
      "Epoch [3/10], Step [310/579], Loss: 0.135969\n",
      "Epoch [3/10], Step [320/579], Loss: 0.455601\n",
      "Epoch [3/10], Step [330/579], Loss: 0.239303\n",
      "Epoch [3/10], Step [340/579], Loss: 0.201247\n",
      "Epoch [3/10], Step [350/579], Loss: 0.569589\n",
      "Epoch [3/10], Step [360/579], Loss: 0.298910\n",
      "Epoch [3/10], Step [370/579], Loss: 0.318672\n",
      "Epoch [3/10], Step [380/579], Loss: 0.010558\n",
      "Epoch [3/10], Step [390/579], Loss: 0.041144\n",
      "Epoch [3/10], Step [400/579], Loss: 0.125821\n",
      "Epoch [3/10], Step [410/579], Loss: 0.218217\n",
      "Epoch [3/10], Step [420/579], Loss: 0.178979\n",
      "Epoch [3/10], Step [430/579], Loss: 0.097127\n",
      "Epoch [3/10], Step [440/579], Loss: 0.090141\n",
      "Epoch [3/10], Step [450/579], Loss: 0.067806\n",
      "Epoch [3/10], Step [460/579], Loss: 0.355167\n",
      "Epoch [3/10], Step [470/579], Loss: 0.523876\n",
      "Epoch [3/10], Step [480/579], Loss: 0.018644\n",
      "Epoch [3/10], Step [490/579], Loss: 0.015967\n",
      "Epoch [3/10], Step [500/579], Loss: 0.360580\n",
      "Epoch [3/10], Step [510/579], Loss: 0.538603\n",
      "Epoch [3/10], Step [520/579], Loss: 0.266064\n",
      "Epoch [3/10], Step [530/579], Loss: 0.664847\n",
      "Epoch [3/10], Step [540/579], Loss: 0.308370\n",
      "Epoch [3/10], Step [550/579], Loss: 0.449846\n",
      "Epoch [3/10], Step [560/579], Loss: 0.412856\n",
      "Epoch [3/10], Step [570/579], Loss: 0.082150\n",
      "[Epoch 3/10], Validation Loss: 0.003358\n",
      "Epoch [4/10], Step [10/579], Loss: 0.167191\n",
      "Epoch [4/10], Step [20/579], Loss: 0.390582\n",
      "Epoch [4/10], Step [30/579], Loss: 0.446167\n",
      "Epoch [4/10], Step [40/579], Loss: 0.038990\n",
      "Epoch [4/10], Step [50/579], Loss: 0.436284\n",
      "Epoch [4/10], Step [60/579], Loss: 0.508929\n",
      "Epoch [4/10], Step [70/579], Loss: 0.070457\n",
      "Epoch [4/10], Step [80/579], Loss: 0.231048\n",
      "Epoch [4/10], Step [90/579], Loss: 0.477091\n",
      "Epoch [4/10], Step [100/579], Loss: 0.384687\n",
      "Epoch [4/10], Step [110/579], Loss: 0.665698\n",
      "Epoch [4/10], Step [120/579], Loss: 0.399184\n",
      "Epoch [4/10], Step [130/579], Loss: 0.085320\n",
      "Epoch [4/10], Step [140/579], Loss: 0.124109\n",
      "Epoch [4/10], Step [150/579], Loss: 0.070280\n",
      "Epoch [4/10], Step [160/579], Loss: 0.034986\n",
      "Epoch [4/10], Step [170/579], Loss: 0.041186\n",
      "Epoch [4/10], Step [180/579], Loss: 0.451172\n",
      "Epoch [4/10], Step [190/579], Loss: 0.820815\n",
      "Epoch [4/10], Step [200/579], Loss: 0.026871\n",
      "Epoch [4/10], Step [210/579], Loss: 0.474308\n",
      "Epoch [4/10], Step [220/579], Loss: 0.144047\n",
      "Epoch [4/10], Step [230/579], Loss: 0.199917\n",
      "Epoch [4/10], Step [240/579], Loss: 0.099617\n",
      "Epoch [4/10], Step [250/579], Loss: 0.140813\n",
      "Epoch [4/10], Step [260/579], Loss: 0.013598\n",
      "Epoch [4/10], Step [270/579], Loss: 0.024982\n",
      "Epoch [4/10], Step [280/579], Loss: 0.250971\n",
      "Epoch [4/10], Step [290/579], Loss: 0.100356\n",
      "Epoch [4/10], Step [300/579], Loss: 0.223560\n",
      "Epoch [4/10], Step [310/579], Loss: 0.175411\n",
      "Epoch [4/10], Step [320/579], Loss: 0.301075\n",
      "Epoch [4/10], Step [330/579], Loss: 0.340083\n",
      "Epoch [4/10], Step [340/579], Loss: 0.421934\n",
      "Epoch [4/10], Step [350/579], Loss: 0.123662\n",
      "Epoch [4/10], Step [360/579], Loss: 0.079938\n",
      "Epoch [4/10], Step [370/579], Loss: 0.404626\n",
      "Epoch [4/10], Step [380/579], Loss: 0.051302\n",
      "Epoch [4/10], Step [390/579], Loss: 0.068220\n",
      "Epoch [4/10], Step [400/579], Loss: 0.144758\n",
      "Epoch [4/10], Step [410/579], Loss: 0.212107\n",
      "Epoch [4/10], Step [420/579], Loss: 0.174976\n",
      "Epoch [4/10], Step [430/579], Loss: 0.022941\n",
      "Epoch [4/10], Step [440/579], Loss: 0.176205\n",
      "Epoch [4/10], Step [450/579], Loss: 0.126332\n",
      "Epoch [4/10], Step [460/579], Loss: 0.259453\n",
      "Epoch [4/10], Step [470/579], Loss: 0.205111\n",
      "Epoch [4/10], Step [480/579], Loss: 0.791559\n",
      "Epoch [4/10], Step [490/579], Loss: 0.055677\n",
      "Epoch [4/10], Step [500/579], Loss: 0.545725\n",
      "Epoch [4/10], Step [510/579], Loss: 0.045822\n",
      "Epoch [4/10], Step [520/579], Loss: 0.671561\n",
      "Epoch [4/10], Step [530/579], Loss: 0.225019\n",
      "Epoch [4/10], Step [540/579], Loss: 0.088014\n",
      "Epoch [4/10], Step [550/579], Loss: 0.562660\n",
      "Epoch [4/10], Step [560/579], Loss: 0.016904\n",
      "Epoch [4/10], Step [570/579], Loss: 0.864258\n",
      "[Epoch 4/10], Validation Loss: 0.003323\n",
      "Epoch [5/10], Step [10/579], Loss: 0.216246\n",
      "Epoch [5/10], Step [20/579], Loss: 0.621949\n",
      "Epoch [5/10], Step [30/579], Loss: 0.200842\n",
      "Epoch [5/10], Step [40/579], Loss: 0.609429\n",
      "Epoch [5/10], Step [50/579], Loss: 0.013991\n",
      "Epoch [5/10], Step [60/579], Loss: 0.310378\n",
      "Epoch [5/10], Step [70/579], Loss: 0.347130\n",
      "Epoch [5/10], Step [80/579], Loss: 0.286595\n",
      "Epoch [5/10], Step [90/579], Loss: 0.659761\n",
      "Epoch [5/10], Step [100/579], Loss: 0.065549\n",
      "Epoch [5/10], Step [110/579], Loss: 0.091049\n",
      "Epoch [5/10], Step [120/579], Loss: 0.101849\n",
      "Epoch [5/10], Step [130/579], Loss: 0.423910\n",
      "Epoch [5/10], Step [140/579], Loss: 0.028317\n",
      "Epoch [5/10], Step [150/579], Loss: 0.384079\n",
      "Epoch [5/10], Step [160/579], Loss: 0.138644\n",
      "Epoch [5/10], Step [170/579], Loss: 0.066062\n",
      "Epoch [5/10], Step [180/579], Loss: 0.294159\n",
      "Epoch [5/10], Step [190/579], Loss: 0.264682\n",
      "Epoch [5/10], Step [200/579], Loss: 0.014201\n",
      "Epoch [5/10], Step [210/579], Loss: 0.363571\n",
      "Epoch [5/10], Step [220/579], Loss: 0.092051\n",
      "Epoch [5/10], Step [230/579], Loss: 0.515381\n",
      "Epoch [5/10], Step [240/579], Loss: 0.092055\n",
      "Epoch [5/10], Step [250/579], Loss: 0.442020\n",
      "Epoch [5/10], Step [260/579], Loss: 0.132909\n",
      "Epoch [5/10], Step [270/579], Loss: 0.513276\n",
      "Epoch [5/10], Step [280/579], Loss: 0.603046\n",
      "Epoch [5/10], Step [290/579], Loss: 0.114329\n",
      "Epoch [5/10], Step [300/579], Loss: 0.108957\n",
      "Epoch [5/10], Step [310/579], Loss: 0.184327\n",
      "Epoch [5/10], Step [320/579], Loss: 0.209340\n",
      "Epoch [5/10], Step [330/579], Loss: 0.364281\n",
      "Epoch [5/10], Step [340/579], Loss: 0.096308\n",
      "Epoch [5/10], Step [350/579], Loss: 0.242558\n",
      "Epoch [5/10], Step [360/579], Loss: 0.099987\n",
      "Epoch [5/10], Step [370/579], Loss: 0.020808\n",
      "Epoch [5/10], Step [380/579], Loss: 0.730471\n",
      "Epoch [5/10], Step [390/579], Loss: 0.508524\n",
      "Epoch [5/10], Step [400/579], Loss: 0.434533\n",
      "Epoch [5/10], Step [410/579], Loss: 0.026150\n",
      "Epoch [5/10], Step [420/579], Loss: 0.594624\n",
      "Epoch [5/10], Step [430/579], Loss: 0.416532\n",
      "Epoch [5/10], Step [440/579], Loss: 0.584558\n",
      "Epoch [5/10], Step [450/579], Loss: 0.492737\n",
      "Epoch [5/10], Step [460/579], Loss: 0.386474\n",
      "Epoch [5/10], Step [470/579], Loss: 0.105609\n",
      "Epoch [5/10], Step [480/579], Loss: 0.584990\n",
      "Epoch [5/10], Step [490/579], Loss: 0.266372\n",
      "Epoch [5/10], Step [500/579], Loss: 0.747382\n",
      "Epoch [5/10], Step [510/579], Loss: 0.707061\n",
      "Epoch [5/10], Step [520/579], Loss: 0.730202\n",
      "Epoch [5/10], Step [530/579], Loss: 0.099627\n",
      "Epoch [5/10], Step [540/579], Loss: 0.226084\n",
      "Epoch [5/10], Step [550/579], Loss: 0.105473\n",
      "Epoch [5/10], Step [560/579], Loss: 0.533409\n",
      "Epoch [5/10], Step [570/579], Loss: 0.172433\n",
      "[Epoch 5/10], Validation Loss: 0.003484\n",
      "Epoch [6/10], Step [10/579], Loss: 0.298537\n",
      "Epoch [6/10], Step [20/579], Loss: 0.475782\n",
      "Epoch [6/10], Step [30/579], Loss: 0.066322\n",
      "Epoch [6/10], Step [40/579], Loss: 0.077150\n",
      "Epoch [6/10], Step [50/579], Loss: 0.141043\n",
      "Epoch [6/10], Step [60/579], Loss: 0.026907\n",
      "Epoch [6/10], Step [70/579], Loss: 0.042578\n",
      "Epoch [6/10], Step [80/579], Loss: 0.548697\n",
      "Epoch [6/10], Step [90/579], Loss: 0.018075\n",
      "Epoch [6/10], Step [100/579], Loss: 0.472515\n",
      "Epoch [6/10], Step [110/579], Loss: 0.378984\n",
      "Epoch [6/10], Step [120/579], Loss: 0.218335\n",
      "Epoch [6/10], Step [130/579], Loss: 0.197959\n",
      "Epoch [6/10], Step [140/579], Loss: 0.374006\n",
      "Epoch [6/10], Step [150/579], Loss: 0.691429\n",
      "Epoch [6/10], Step [160/579], Loss: 0.019388\n",
      "Epoch [6/10], Step [170/579], Loss: 0.016678\n",
      "Epoch [6/10], Step [180/579], Loss: 0.469493\n",
      "Epoch [6/10], Step [190/579], Loss: 0.323192\n",
      "Epoch [6/10], Step [200/579], Loss: 0.528074\n",
      "Epoch [6/10], Step [210/579], Loss: 0.291178\n",
      "Epoch [6/10], Step [220/579], Loss: 0.116184\n",
      "Epoch [6/10], Step [230/579], Loss: 0.533761\n",
      "Epoch [6/10], Step [240/579], Loss: 0.529766\n",
      "Epoch [6/10], Step [250/579], Loss: 0.391723\n",
      "Epoch [6/10], Step [260/579], Loss: 0.212491\n",
      "Epoch [6/10], Step [270/579], Loss: 0.121071\n",
      "Epoch [6/10], Step [280/579], Loss: 0.884330\n",
      "Epoch [6/10], Step [290/579], Loss: 0.062435\n",
      "Epoch [6/10], Step [300/579], Loss: 0.502328\n",
      "Epoch [6/10], Step [310/579], Loss: 0.024479\n",
      "Epoch [6/10], Step [320/579], Loss: 0.013710\n",
      "Epoch [6/10], Step [330/579], Loss: 0.404516\n",
      "Epoch [6/10], Step [340/579], Loss: 0.065018\n",
      "Epoch [6/10], Step [350/579], Loss: 0.394095\n",
      "Epoch [6/10], Step [360/579], Loss: 0.259368\n",
      "Epoch [6/10], Step [370/579], Loss: 0.118323\n",
      "Epoch [6/10], Step [380/579], Loss: 0.668913\n",
      "Epoch [6/10], Step [390/579], Loss: 0.303257\n",
      "Epoch [6/10], Step [400/579], Loss: 0.564642\n",
      "Epoch [6/10], Step [410/579], Loss: 0.605657\n",
      "Epoch [6/10], Step [420/579], Loss: 0.137574\n",
      "Epoch [6/10], Step [430/579], Loss: 0.063601\n",
      "Epoch [6/10], Step [440/579], Loss: 0.266605\n",
      "Epoch [6/10], Step [450/579], Loss: 0.306372\n",
      "Epoch [6/10], Step [460/579], Loss: 0.464691\n",
      "Epoch [6/10], Step [470/579], Loss: 0.416714\n",
      "Epoch [6/10], Step [480/579], Loss: 0.061621\n",
      "Epoch [6/10], Step [490/579], Loss: 0.082920\n",
      "Epoch [6/10], Step [500/579], Loss: 0.136470\n",
      "Epoch [6/10], Step [510/579], Loss: 0.143305\n",
      "Epoch [6/10], Step [520/579], Loss: 0.530190\n",
      "Epoch [6/10], Step [530/579], Loss: 0.163216\n",
      "Epoch [6/10], Step [540/579], Loss: 0.014086\n",
      "Epoch [6/10], Step [550/579], Loss: 0.271880\n",
      "Epoch [6/10], Step [560/579], Loss: 0.092032\n",
      "Epoch [6/10], Step [570/579], Loss: 0.342427\n",
      "[Epoch 6/10], Validation Loss: 0.003463\n",
      "Epoch [7/10], Step [10/579], Loss: 0.271143\n",
      "Epoch [7/10], Step [20/579], Loss: 0.091474\n",
      "Epoch [7/10], Step [30/579], Loss: 0.083887\n",
      "Epoch [7/10], Step [40/579], Loss: 0.462034\n",
      "Epoch [7/10], Step [50/579], Loss: 0.018652\n",
      "Epoch [7/10], Step [60/579], Loss: 0.145765\n",
      "Epoch [7/10], Step [70/579], Loss: 0.035005\n",
      "Epoch [7/10], Step [80/579], Loss: 0.467948\n",
      "Epoch [7/10], Step [90/579], Loss: 0.630050\n",
      "Epoch [7/10], Step [100/579], Loss: 0.381768\n",
      "Epoch [7/10], Step [110/579], Loss: 0.073995\n",
      "Epoch [7/10], Step [120/579], Loss: 0.105466\n",
      "Epoch [7/10], Step [130/579], Loss: 0.333659\n",
      "Epoch [7/10], Step [140/579], Loss: 0.075139\n",
      "Epoch [7/10], Step [150/579], Loss: 0.031570\n",
      "Epoch [7/10], Step [160/579], Loss: 0.073672\n",
      "Epoch [7/10], Step [170/579], Loss: 0.541996\n",
      "Epoch [7/10], Step [180/579], Loss: 0.067321\n",
      "Epoch [7/10], Step [190/579], Loss: 0.303988\n",
      "Epoch [7/10], Step [200/579], Loss: 0.478621\n",
      "Epoch [7/10], Step [210/579], Loss: 0.423753\n",
      "Epoch [7/10], Step [220/579], Loss: 0.090204\n",
      "Epoch [7/10], Step [230/579], Loss: 0.027936\n",
      "Epoch [7/10], Step [240/579], Loss: 0.040481\n",
      "Epoch [7/10], Step [250/579], Loss: 0.564914\n",
      "Epoch [7/10], Step [260/579], Loss: 0.030494\n",
      "Epoch [7/10], Step [270/579], Loss: 0.126827\n",
      "Epoch [7/10], Step [280/579], Loss: 0.107295\n",
      "Epoch [7/10], Step [290/579], Loss: 0.315933\n",
      "Epoch [7/10], Step [300/579], Loss: 0.416762\n",
      "Epoch [7/10], Step [310/579], Loss: 0.543405\n",
      "Epoch [7/10], Step [320/579], Loss: 0.083825\n",
      "Epoch [7/10], Step [330/579], Loss: 0.173134\n",
      "Epoch [7/10], Step [340/579], Loss: 0.382513\n",
      "Epoch [7/10], Step [350/579], Loss: 0.041195\n",
      "Epoch [7/10], Step [360/579], Loss: 0.588812\n",
      "Epoch [7/10], Step [370/579], Loss: 0.114067\n",
      "Epoch [7/10], Step [380/579], Loss: 0.205869\n",
      "Epoch [7/10], Step [390/579], Loss: 0.162753\n",
      "Epoch [7/10], Step [400/579], Loss: 0.092232\n",
      "Epoch [7/10], Step [410/579], Loss: 0.438820\n",
      "Epoch [7/10], Step [420/579], Loss: 0.222347\n",
      "Epoch [7/10], Step [430/579], Loss: 0.376679\n",
      "Epoch [7/10], Step [440/579], Loss: 0.067492\n",
      "Epoch [7/10], Step [450/579], Loss: 0.194800\n",
      "Epoch [7/10], Step [460/579], Loss: 0.282275\n",
      "Epoch [7/10], Step [470/579], Loss: 0.571071\n",
      "Epoch [7/10], Step [480/579], Loss: 0.052000\n",
      "Epoch [7/10], Step [490/579], Loss: 0.096330\n",
      "Epoch [7/10], Step [500/579], Loss: 0.670798\n",
      "Epoch [7/10], Step [510/579], Loss: 0.150239\n",
      "Epoch [7/10], Step [520/579], Loss: 0.479319\n",
      "Epoch [7/10], Step [530/579], Loss: 0.330599\n",
      "Epoch [7/10], Step [540/579], Loss: 0.127413\n",
      "Epoch [7/10], Step [550/579], Loss: 0.124017\n",
      "Epoch [7/10], Step [560/579], Loss: 0.027586\n",
      "Epoch [7/10], Step [570/579], Loss: 0.109063\n",
      "[Epoch 7/10], Validation Loss: 0.003557\n",
      "Epoch [8/10], Step [10/579], Loss: 0.411221\n",
      "Epoch [8/10], Step [20/579], Loss: 0.101295\n",
      "Epoch [8/10], Step [30/579], Loss: 0.168471\n",
      "Epoch [8/10], Step [40/579], Loss: 0.162141\n",
      "Epoch [8/10], Step [50/579], Loss: 0.141214\n",
      "Epoch [8/10], Step [60/579], Loss: 0.706411\n",
      "Epoch [8/10], Step [70/579], Loss: 0.279691\n",
      "Epoch [8/10], Step [80/579], Loss: 0.090464\n",
      "Epoch [8/10], Step [90/579], Loss: 0.515273\n",
      "Epoch [8/10], Step [100/579], Loss: 0.434081\n",
      "Epoch [8/10], Step [110/579], Loss: 0.065561\n",
      "Epoch [8/10], Step [120/579], Loss: 0.199432\n",
      "Epoch [8/10], Step [130/579], Loss: 0.176840\n",
      "Epoch [8/10], Step [140/579], Loss: 0.164169\n",
      "Epoch [8/10], Step [150/579], Loss: 0.047347\n",
      "Epoch [8/10], Step [160/579], Loss: 0.095167\n",
      "Epoch [8/10], Step [170/579], Loss: 0.255470\n",
      "Epoch [8/10], Step [180/579], Loss: 0.407702\n",
      "Epoch [8/10], Step [190/579], Loss: 0.403296\n",
      "Epoch [8/10], Step [200/579], Loss: 0.161270\n",
      "Epoch [8/10], Step [210/579], Loss: 0.201189\n",
      "Epoch [8/10], Step [220/579], Loss: 0.517917\n",
      "Epoch [8/10], Step [230/579], Loss: 0.662233\n",
      "Epoch [8/10], Step [240/579], Loss: 0.712477\n",
      "Epoch [8/10], Step [250/579], Loss: 0.684994\n",
      "Epoch [8/10], Step [260/579], Loss: 0.192368\n",
      "Epoch [8/10], Step [270/579], Loss: 0.013248\n",
      "Epoch [8/10], Step [280/579], Loss: 0.183940\n",
      "Epoch [8/10], Step [290/579], Loss: 0.859179\n",
      "Epoch [8/10], Step [300/579], Loss: 0.352298\n",
      "Epoch [8/10], Step [310/579], Loss: 0.464652\n",
      "Epoch [8/10], Step [320/579], Loss: 0.126712\n",
      "Epoch [8/10], Step [330/579], Loss: 0.862351\n",
      "Epoch [8/10], Step [340/579], Loss: 0.480148\n",
      "Epoch [8/10], Step [350/579], Loss: 0.009533\n",
      "Epoch [8/10], Step [360/579], Loss: 0.033876\n",
      "Epoch [8/10], Step [370/579], Loss: 0.032437\n",
      "Epoch [8/10], Step [380/579], Loss: 0.029627\n",
      "Epoch [8/10], Step [390/579], Loss: 0.320592\n",
      "Epoch [8/10], Step [400/579], Loss: 0.036170\n",
      "Epoch [8/10], Step [410/579], Loss: 0.150976\n",
      "Epoch [8/10], Step [420/579], Loss: 0.051146\n",
      "Epoch [8/10], Step [430/579], Loss: 0.696068\n",
      "Epoch [8/10], Step [440/579], Loss: 0.406933\n",
      "Epoch [8/10], Step [450/579], Loss: 0.085505\n",
      "Epoch [8/10], Step [460/579], Loss: 0.211183\n",
      "Epoch [8/10], Step [470/579], Loss: 0.556972\n",
      "Epoch [8/10], Step [480/579], Loss: 0.031713\n",
      "Epoch [8/10], Step [490/579], Loss: 0.095004\n",
      "Epoch [8/10], Step [500/579], Loss: 0.465395\n",
      "Epoch [8/10], Step [510/579], Loss: 0.179599\n",
      "Epoch [8/10], Step [520/579], Loss: 0.395270\n",
      "Epoch [8/10], Step [530/579], Loss: 0.330029\n",
      "Epoch [8/10], Step [540/579], Loss: 0.216899\n",
      "Epoch [8/10], Step [550/579], Loss: 0.353286\n",
      "Epoch [8/10], Step [560/579], Loss: 0.098447\n",
      "Epoch [8/10], Step [570/579], Loss: 0.049855\n",
      "[Epoch 8/10], Validation Loss: 0.003399\n",
      "Epoch [9/10], Step [10/579], Loss: 0.110190\n",
      "Epoch [9/10], Step [20/579], Loss: 0.706302\n",
      "Epoch [9/10], Step [30/579], Loss: 0.456605\n",
      "Epoch [9/10], Step [40/579], Loss: 0.494290\n",
      "Epoch [9/10], Step [50/579], Loss: 0.502284\n",
      "Epoch [9/10], Step [60/579], Loss: 0.623501\n",
      "Epoch [9/10], Step [70/579], Loss: 0.744519\n",
      "Epoch [9/10], Step [80/579], Loss: 0.205930\n",
      "Epoch [9/10], Step [90/579], Loss: 0.156736\n",
      "Epoch [9/10], Step [100/579], Loss: 0.082311\n",
      "Epoch [9/10], Step [110/579], Loss: 0.028343\n",
      "Epoch [9/10], Step [120/579], Loss: 0.008434\n",
      "Epoch [9/10], Step [130/579], Loss: 0.026420\n",
      "Epoch [9/10], Step [140/579], Loss: 0.090028\n",
      "Epoch [9/10], Step [150/579], Loss: 0.522146\n",
      "Epoch [9/10], Step [160/579], Loss: 0.281092\n",
      "Epoch [9/10], Step [170/579], Loss: 0.082736\n",
      "Epoch [9/10], Step [180/579], Loss: 0.598125\n",
      "Epoch [9/10], Step [190/579], Loss: 0.903325\n",
      "Epoch [9/10], Step [200/579], Loss: 0.200511\n",
      "Epoch [9/10], Step [210/579], Loss: 0.023008\n",
      "Epoch [9/10], Step [220/579], Loss: 0.145997\n",
      "Epoch [9/10], Step [230/579], Loss: 0.301811\n",
      "Epoch [9/10], Step [240/579], Loss: 0.008566\n",
      "Epoch [9/10], Step [250/579], Loss: 0.933708\n",
      "Epoch [9/10], Step [260/579], Loss: 0.636585\n",
      "Epoch [9/10], Step [270/579], Loss: 0.235643\n",
      "Epoch [9/10], Step [280/579], Loss: 0.574688\n",
      "Epoch [9/10], Step [290/579], Loss: 0.183266\n",
      "Epoch [9/10], Step [300/579], Loss: 0.150632\n",
      "Epoch [9/10], Step [310/579], Loss: 0.464069\n",
      "Epoch [9/10], Step [320/579], Loss: 0.051409\n",
      "Epoch [9/10], Step [330/579], Loss: 0.179302\n",
      "Epoch [9/10], Step [340/579], Loss: 0.403542\n",
      "Epoch [9/10], Step [350/579], Loss: 0.113961\n",
      "Epoch [9/10], Step [360/579], Loss: 0.372958\n",
      "Epoch [9/10], Step [370/579], Loss: 0.092987\n",
      "Epoch [9/10], Step [380/579], Loss: 0.127862\n",
      "Epoch [9/10], Step [390/579], Loss: 0.873707\n",
      "Epoch [9/10], Step [400/579], Loss: 0.400599\n",
      "Epoch [9/10], Step [410/579], Loss: 0.685875\n",
      "Epoch [9/10], Step [420/579], Loss: 0.088527\n",
      "Epoch [9/10], Step [430/579], Loss: 0.202927\n",
      "Epoch [9/10], Step [440/579], Loss: 0.484274\n",
      "Epoch [9/10], Step [450/579], Loss: 0.123720\n",
      "Epoch [9/10], Step [460/579], Loss: 0.309552\n",
      "Epoch [9/10], Step [470/579], Loss: 0.406924\n",
      "Epoch [9/10], Step [480/579], Loss: 0.552579\n",
      "Epoch [9/10], Step [490/579], Loss: 0.100055\n",
      "Epoch [9/10], Step [500/579], Loss: 0.132486\n",
      "Epoch [9/10], Step [510/579], Loss: 0.070644\n",
      "Epoch [9/10], Step [520/579], Loss: 0.119050\n",
      "Epoch [9/10], Step [530/579], Loss: 0.326175\n",
      "Epoch [9/10], Step [540/579], Loss: 0.189872\n",
      "Epoch [9/10], Step [550/579], Loss: 0.647929\n",
      "Epoch [9/10], Step [560/579], Loss: 0.072397\n",
      "Epoch [9/10], Step [570/579], Loss: 0.502795\n",
      "[Epoch 9/10], Validation Loss: 0.003345\n",
      "Epoch [10/10], Step [10/579], Loss: 0.078530\n",
      "Epoch [10/10], Step [20/579], Loss: 0.716187\n",
      "Epoch [10/10], Step [30/579], Loss: 0.016347\n",
      "Epoch [10/10], Step [40/579], Loss: 0.249355\n",
      "Epoch [10/10], Step [50/579], Loss: 0.205028\n",
      "Epoch [10/10], Step [60/579], Loss: 0.098336\n",
      "Epoch [10/10], Step [70/579], Loss: 0.069942\n",
      "Epoch [10/10], Step [80/579], Loss: 0.341083\n",
      "Epoch [10/10], Step [90/579], Loss: 0.027124\n",
      "Epoch [10/10], Step [100/579], Loss: 0.106289\n",
      "Epoch [10/10], Step [110/579], Loss: 0.023421\n",
      "Epoch [10/10], Step [120/579], Loss: 0.702529\n",
      "Epoch [10/10], Step [130/579], Loss: 0.318246\n",
      "Epoch [10/10], Step [140/579], Loss: 0.017306\n",
      "Epoch [10/10], Step [150/579], Loss: 0.181371\n",
      "Epoch [10/10], Step [160/579], Loss: 0.140204\n",
      "Epoch [10/10], Step [170/579], Loss: 0.190772\n",
      "Epoch [10/10], Step [180/579], Loss: 0.194428\n",
      "Epoch [10/10], Step [190/579], Loss: 0.126889\n",
      "Epoch [10/10], Step [200/579], Loss: 0.099439\n",
      "Epoch [10/10], Step [210/579], Loss: 0.127414\n",
      "Epoch [10/10], Step [220/579], Loss: 0.631827\n",
      "Epoch [10/10], Step [230/579], Loss: 0.072093\n",
      "Epoch [10/10], Step [240/579], Loss: 0.024250\n",
      "Epoch [10/10], Step [250/579], Loss: 0.192958\n",
      "Epoch [10/10], Step [260/579], Loss: 0.032885\n",
      "Epoch [10/10], Step [270/579], Loss: 0.190572\n",
      "Epoch [10/10], Step [280/579], Loss: 0.106431\n",
      "Epoch [10/10], Step [290/579], Loss: 0.342839\n",
      "Epoch [10/10], Step [300/579], Loss: 0.093450\n",
      "Epoch [10/10], Step [310/579], Loss: 0.090396\n",
      "Epoch [10/10], Step [320/579], Loss: 0.144903\n",
      "Epoch [10/10], Step [330/579], Loss: 0.305447\n",
      "Epoch [10/10], Step [340/579], Loss: 0.186666\n",
      "Epoch [10/10], Step [350/579], Loss: 0.696633\n",
      "Epoch [10/10], Step [360/579], Loss: 0.285551\n",
      "Epoch [10/10], Step [370/579], Loss: 0.580025\n",
      "Epoch [10/10], Step [380/579], Loss: 0.444929\n",
      "Epoch [10/10], Step [390/579], Loss: 0.133638\n",
      "Epoch [10/10], Step [400/579], Loss: 0.453848\n",
      "Epoch [10/10], Step [410/579], Loss: 0.406596\n",
      "Epoch [10/10], Step [420/579], Loss: 0.582641\n",
      "Epoch [10/10], Step [430/579], Loss: 0.103567\n",
      "Epoch [10/10], Step [440/579], Loss: 0.522269\n",
      "Epoch [10/10], Step [450/579], Loss: 0.415848\n",
      "Epoch [10/10], Step [460/579], Loss: 0.563959\n",
      "Epoch [10/10], Step [470/579], Loss: 0.572363\n",
      "Epoch [10/10], Step [480/579], Loss: 0.285527\n",
      "Epoch [10/10], Step [490/579], Loss: 0.127002\n",
      "Epoch [10/10], Step [500/579], Loss: 0.084073\n",
      "Epoch [10/10], Step [510/579], Loss: 0.599424\n",
      "Epoch [10/10], Step [520/579], Loss: 0.038543\n",
      "Epoch [10/10], Step [530/579], Loss: 0.048170\n",
      "Epoch [10/10], Step [540/579], Loss: 0.549534\n",
      "Epoch [10/10], Step [550/579], Loss: 0.025256\n",
      "Epoch [10/10], Step [560/579], Loss: 0.355526\n",
      "Epoch [10/10], Step [570/579], Loss: 0.449332\n",
      "[Epoch 10/10], Validation Loss: 0.003457\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004, weight_decay=1e-6)\n",
    "\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    for index, (ft_item, gt_item) in enumerate(train_loader):\n",
    "        ft_item = ft_item.cuda().float()\n",
    "        gt_item = gt_item.cuda().float()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output_item = model(ft_item)\n",
    "        loss = loss_func(output_item, gt_item)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += loss.item()\n",
    "        # Print the loss for every 10 steps\n",
    "        if (index+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{index+1}/{len(train_loader)}], Loss: {loss.item():.6f}\")\n",
    "            loss = 0.0\n",
    "    # Save the model weights\n",
    "    torch.save(model.state_dict(), './model/model_weights.pth')\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for index, (ft_item, gt_item) in enumerate(val_loader):\n",
    "            ft_item = ft_item.cuda().float()\n",
    "            gt_item = gt_item.cuda().float()\n",
    "            output_item = model(ft_item)\n",
    "            val_loss = loss_func(output_item, gt_item)\n",
    "            val_loss += val_loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}], Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **第十步**\n",
    "- 模型推理部分, 通过加载模型使用测试数据作为输入, 得到预测结果\n",
    "- 其中test_data_path需要给出从下载测试数据解压后的目录路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape for sample 000: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 001: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 002: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 003: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 004: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 005: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 006: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 007: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 008: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 009: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 010: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 011: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 012: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 013: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 014: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 015: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 016: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 017: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 018: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 019: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 020: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 021: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 022: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 023: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 024: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 025: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 026: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 027: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 028: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 029: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 030: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 031: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 032: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 033: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 034: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 035: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 036: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 037: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 038: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 039: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 040: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 041: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 042: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 043: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 044: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 045: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 046: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 047: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 048: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 049: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 050: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 051: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 052: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 053: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 054: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 055: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 056: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 057: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 058: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 059: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 060: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 061: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 062: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 063: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 064: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 065: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 066: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 067: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 068: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 069: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 070: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 071: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 072: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 073: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 074: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 075: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 076: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 077: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 078: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 079: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 080: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 081: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 082: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 083: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 084: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 085: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 086: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 087: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 088: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 089: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 090: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 091: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 092: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 093: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 094: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 095: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 096: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 097: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 098: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 099: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 100: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 101: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 102: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 103: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 104: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 105: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 106: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 107: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 108: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 109: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 110: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 111: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 112: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 113: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 114: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 115: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 116: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 117: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 118: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 119: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 120: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 121: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 122: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 123: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 124: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 125: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 126: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 127: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 128: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 129: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 130: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 131: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 132: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 133: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 134: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 135: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 136: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 137: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 138: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 139: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 140: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 141: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 142: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 143: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 144: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 145: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 146: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 147: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 148: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 149: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 150: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 151: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 152: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 153: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 154: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 155: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 156: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 157: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 158: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 159: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 160: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 161: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 162: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 163: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 164: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 165: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 166: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 167: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 168: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 169: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 170: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 171: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 172: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 173: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 174: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 175: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 176: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 177: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 178: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 179: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 180: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 181: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 182: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 183: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 184: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 185: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 186: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 187: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 188: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 189: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 190: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 191: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 192: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 193: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 194: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 195: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 196: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 197: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 198: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 199: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 200: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 201: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 202: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 203: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 204: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 205: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 206: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 207: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 208: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 209: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 210: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 211: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 212: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 213: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 214: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 215: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 216: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 217: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 218: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 219: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 220: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 221: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 222: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 223: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 224: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 225: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 226: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 227: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 228: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 229: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 230: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 231: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 232: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 233: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 234: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 235: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 236: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 237: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 238: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 239: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 240: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 241: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 242: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 243: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 244: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 245: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 246: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 247: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 248: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 249: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 250: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 251: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 252: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 253: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 254: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 255: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 256: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 257: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 258: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 259: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 260: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 261: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 262: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 263: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 264: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 265: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 266: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 267: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 268: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 269: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 270: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 271: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 272: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 273: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 274: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 275: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 276: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 277: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 278: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 279: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 280: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 281: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 282: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 283: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 284: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 285: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 286: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 287: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 288: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 289: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 290: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 291: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 292: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 293: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 294: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 295: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 296: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 297: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 298: torch.Size([1, 72, 57, 81]), True\n",
      "Output shape for sample 299: torch.Size([1, 72, 57, 81]), True\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('./model/model_weights.pth'))\n",
    "model.eval()\n",
    "import os\n",
    "\n",
    "test_data_path = \"/ai_share/caozhibin/tianchi_precipatition_prediction/test\"\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "for index, test_data_file in enumerate(os.listdir(test_data_path)):\n",
    "    test_data = torch.load(os.path.join(test_data_path, test_data_file))\n",
    "    test_data = test_data.cuda().float()\n",
    "    \n",
    "    # Forward pass\n",
    "    output_item = model(test_data)\n",
    "    output_item = output_item.to(torch.float16)\n",
    "    \n",
    "    # Print the output shape\n",
    "    print(f\"Output shape for sample {test_data_file.split('.')[0]}: {output_item.shape}, {output_item.dtype == torch.float16}\")\n",
    "    \n",
    "    # Save the output\n",
    "    output_path = f\"output/{test_data_file}\"\n",
    "    torch.save(output_item.cpu(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
